{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA RAPIDS on Azure ML\n",
    "## GTC 2020 DLI WORKSHOP\n",
    "\n",
    "In this notebook we use NYC Taxi dataset to showcase some of the speedup and the ease of converting the single-threaded `pandas` execution with GPU-accellerated ETL workload using `cudf` from RAPIDS.\n",
    "\n",
    "**AUTHORS**\n",
    "* Tom Drabas (Microsoft)\n",
    "* Manuel Reyes Gomez (NVIDIA)\n",
    "\n",
    "\n",
    "**GREATER TEAM**\n",
    "* Joshua Patterson (NVIDIA)\n",
    "* Keith Kraus (NVIDIA)\n",
    "* Brad Rees (NVIDIA)\n",
    "* John Zedlewski (NVIDIA)\n",
    "* Paul Mahler (NVIDIA)\n",
    "* Nick Becker (NVIDIA)\n",
    "* Michael Beaumont (NVIDIA)\n",
    "* Chau Dang (NVIDIA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cudf\n",
    "import dask_cudf\n",
    "import dask\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define global vars and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_dtypes = OrderedDict(\n",
    "    [\n",
    "        ('vendor_id', 'int32'),\n",
    "        ('pickup_datetime', 'date'),\n",
    "        ('dropoff_datetime', 'date'),\n",
    "        ('passenger_count', 'int32'),\n",
    "        ('trip_distance', 'int32'),\n",
    "        ('pickup_longitude', 'float64'),\n",
    "        ('pickup_latitude', 'float64'),\n",
    "        ('rate_code', 'int32'),\n",
    "        ('store_and_fwd_flag', 'int32'),\n",
    "        ('dropoff_longitude', 'float64'),\n",
    "        ('dropoff_latitude', 'float64'),\n",
    "        ('payment_type', 'int32'),\n",
    "        ('fare_amount', 'float64'),\n",
    "        ('extra', 'float64'),\n",
    "        ('mta_tax', 'float64'),\n",
    "        ('tip_amount', 'float64'),\n",
    "        ('tolls_amount', 'float64'),\n",
    "        ('surcharge', 'float64'),\n",
    "        ('total_amount', 'float64')\n",
    "    ]\n",
    ")\n",
    "\n",
    "use_col  = [\n",
    "      'pickup_datetime'\n",
    "    , 'dropoff_datetime'\n",
    "    , 'passenger_count'\n",
    "    , 'trip_distance'\n",
    "    , 'pickup_longitude'\n",
    "    , 'pickup_latitude'\n",
    "    , 'rate_code'\n",
    "    , 'dropoff_longitude'\n",
    "    , 'dropoff_latitude'\n",
    "    , 'fare_amount'\n",
    "]\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_message(msg, length=80, filler='#', pre_post=''):\n",
    "    print(f'{pre_post} {msg} {pre_post}'.center(length, filler))\n",
    "    \n",
    "def print_time(t_curr, t_next, t_start, length=80):\n",
    "    print('> Step time: {0}, elapsed time: {1}'\n",
    "          .format(str(t_curr - t_next), str(t_curr - t_start)).rjust(length, '-'))\n",
    "    \n",
    "def haversine_distance_kernel_gpu(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, h_distance):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude)):\n",
    "        x_1 = pi / 180 * x_1\n",
    "        y_1 = pi / 180 * y_1\n",
    "        x_2 = pi / 180 * x_2\n",
    "        y_2 = pi / 180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat / 2)**2 + cos(x_1) * cos(x_2) * sin(dlon / 2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 3959 # Radius of earth in miles\n",
    "        \n",
    "        h_distance[i] = c * r\n",
    "        \n",
    "def haversine_distance_kernel_cpu(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2]) \n",
    "    \n",
    "    newlon = lon2 - lon1\n",
    "    newlat = lat2 - lat1\n",
    " \n",
    "    haver_formula = np.sin(newlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(newlon/2.0)**2\n",
    " \n",
    "    dist = 2 * np.arcsin(np.sqrt(haver_formula ))\n",
    "    km = 3959 * dist\n",
    "    return km\n",
    "    \n",
    "def add_features(df, gpu):\n",
    "    df['pickup_datetime'] = df['pickup_datetime'].astype('datetime64[ms]')\n",
    "    \n",
    "    df['hour']  = df['pickup_datetime'].dt.hour\n",
    "    df['year']  = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day']   = df['pickup_datetime'].dt.day\n",
    "    \n",
    "    df['pickup_latitude_r']   = (df['pickup_latitude']   / .01).astype('int') / 100.0\n",
    "    df['pickup_longitude_r']  = (df['pickup_longitude']  / .01).astype('int') / 100.0\n",
    "    df['dropoff_latitude_r']  = (df['dropoff_latitude']  / .01).astype('int') / 100.0\n",
    "    df['dropoff_longitude_r'] = (df['dropoff_longitude'] / .01).astype('int') / 100.0\n",
    "    \n",
    "    if gpu:\n",
    "        df = df.drop('pickup_datetime')\n",
    "        df = df.drop('dropoff_datetime')\n",
    "        \n",
    "        df = df.apply_rows(\n",
    "            haversine_distance_kernel_gpu\n",
    "            , incols=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']\n",
    "            , outcols=dict(h_distance=np.float32)\n",
    "            , kwargs=dict()\n",
    "        )\n",
    "    else:\n",
    "        df = df.drop('pickup_datetime', axis=1)\n",
    "        df = df.drop('dropoff_datetime', axis=1)\n",
    "        \n",
    "        df['h_distance'] = haversine_distance_kernel_cpu(\n",
    "            df['pickup_latitude']\n",
    "            , df['pickup_longitude']\n",
    "            , df['dropoff_latitude']\n",
    "            , df['dropoff_longitude']\n",
    "        )\n",
    "        \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GPU workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_fares(fare_amount, fare_bin):\n",
    "    for i, fare in enumerate(fare_amount):\n",
    "        fare_bin[i] = int(fare / 10.0) * 10\n",
    "\n",
    "def run_gpu_workflow(data_path, filename):\n",
    "    t_start = datetime.datetime.now()\n",
    "    print_message(f'LOADING DATA: {filename}')\n",
    "    \n",
    "    taxi_df = cudf.read_csv(\n",
    "              os.path.join(data_path, filename)\n",
    "            , names=list(columns_dtypes.keys())\n",
    "            , dtype=list(columns_dtypes.values())\n",
    "            , skiprows=1\n",
    "            , usecols=use_col\n",
    "        )\n",
    "    t_next = datetime.datetime.now()\n",
    "    print_time(t_next, t_start, t_start)\n",
    "    \n",
    "    print()\n",
    "    print_message('NUMBER OF ROWS: {0:,}'.format(len(taxi_df)), pre_post='+', filler='-')\n",
    "    print()\n",
    "    \n",
    "    print_message('SUBSETTING DATA')\n",
    "    # apply a list of filter conditions to throw out records with missing or outlier values\n",
    "    taxi_df = taxi_df.query(' and '.join(query_frags))\n",
    "    t_curr = datetime.datetime.now()\n",
    "    print_time(t_curr, t_next, t_start)\n",
    "    t_next = t_curr\n",
    "    \n",
    "    print_message('FEATURIZING DATA')\n",
    "    taxi_df = add_features(taxi_df, gpu=True)\n",
    "    t_curr = datetime.datetime.now()\n",
    "    print_time(t_curr, t_next, t_start)\n",
    "    t_next = t_curr\n",
    "    \n",
    "    print_message('SORTING DATA')\n",
    "    taxi_df = taxi_df.sort_values(by='fare_amount')\n",
    "    t_curr = datetime.datetime.now()\n",
    "    print_time(t_curr, t_next, t_start)\n",
    "    t_next = t_curr\n",
    "    \n",
    "    print_message('GROUPING DATA')\n",
    "    ### PUT THE FARE IN BINS OF $10\n",
    "    taxi_df = taxi_df.apply_rows(\n",
    "        bin_fares\n",
    "        , incols = {'fare_amount': 'fare_amount'}\n",
    "        , outcols = {'fare_bin': np.int32}\n",
    "        , kwargs = {}\n",
    "    )\n",
    "\n",
    "    taxi_df_fare = taxi_df[['fare_bin', 'passenger_count']].groupby(by='fare_bin').count()\n",
    "    \n",
    "    t_curr = datetime.datetime.now()\n",
    "    print_time(t_curr, t_next, t_start)\n",
    "    \n",
    "    return taxi_df, taxi_df_fare, t_curr - t_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CPU workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cpu_workflow(data_path, filename):\n",
    "    t_start = datetime.datetime.now()\n",
    "    print_message(f'LOADING DATA: {filename}')\n",
    "    \n",
    "    taxi_df = pd.read_csv(\n",
    "              os.path.join(data_path, filename)\n",
    "            , names=list(columns_dtypes.keys())\n",
    "            , parse_dates=True\n",
    "            , skiprows=2\n",
    "            , usecols=use_col\n",
    "        )\n",
    "    t_next = datetime.datetime.now()\n",
    "    print_time(t_next, t_start, t_start)\n",
    "    \n",
    "    print()\n",
    "    print_message('NUMBER OF ROWS: {0:,}'.format(len(taxi_df)), pre_post='+', filler='-')\n",
    "    print()\n",
    "    \n",
    "    print_message('SUBSETTING DATA')\n",
    "    # apply a list of filter conditions to throw out records with missing or outlier values\n",
    "    taxi_df = taxi_df.query(' and '.join(query_frags))\n",
    "    t_curr = datetime.datetime.now()\n",
    "    print_time(t_curr, t_next, t_start)\n",
    "    t_next = t_curr\n",
    "    \n",
    "    print_message('FEATURIZING DATA')\n",
    "    taxi_df = add_features(taxi_df, gpu=False)\n",
    "    t_curr = datetime.datetime.now()\n",
    "    print_time(t_curr, t_next, t_start)\n",
    "    t_next = t_curr\n",
    "    \n",
    "    print_message('SORTING DATA')\n",
    "    taxi_df = taxi_df.sort_values(by='fare_amount')\n",
    "    t_curr = datetime.datetime.now()\n",
    "    print_time(t_curr, t_next, t_start)\n",
    "    t_next = t_curr\n",
    "    \n",
    "    print_message('GROUPING DATA')\n",
    "    ### PUT THE FARE IN BINS OF $10\n",
    "    taxi_df['fare_bin'] = taxi_df.apply(lambda row: int(row['fare_amount'] / 10.0) * 10, axis=1)\n",
    "\n",
    "    taxi_df_fare = taxi_df[['fare_bin', 'passenger_count']].groupby(by='fare_bin').count()\n",
    "    \n",
    "    t_curr = datetime.datetime.now()\n",
    "    print_time(t_curr, t_next, t_start)\n",
    "    \n",
    "    return taxi_df, taxi_df_fare, t_curr - t_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015  2016\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../../../../datafiles/nyctaxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ LOADING DATA: 2016/yellow_tripdata_2016-06.csv ################\n",
      "-----------------------> Step time: 0:00:01.198529, elapsed time: 0:00:01.198529\n",
      "\n",
      "-------------------------+ NUMBER OF ROWS: 11,135,470 +-------------------------\n",
      "\n",
      "############################### SUBSETTING DATA ################################\n",
      "-----------------------> Step time: 0:00:00.047048, elapsed time: 0:00:01.245577\n",
      "############################### FEATURIZING DATA ###############################\n",
      "-----------------------> Step time: 0:00:00.321447, elapsed time: 0:00:01.567024\n",
      "################################# SORTING DATA #################################\n",
      "-----------------------> Step time: 0:00:00.132803, elapsed time: 0:00:01.699827\n",
      "################################ GROUPING DATA #################################\n",
      "-----------------------> Step time: 0:00:00.127722, elapsed time: 0:00:01.827549\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../../../../../../datafiles'     #### REPLACE WITH THE DATA STORE PATH\n",
    "data_path = os.path.join(data_dir, \"nyctaxi\")\n",
    "filename = '2016/yellow_tripdata_2016-06.csv'\n",
    "\n",
    "gpu_results = run_gpu_workflow(data_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################ LOADING DATA: 2016/yellow_tripdata_2016-06.csv ################\n",
      "-----------------------> Step time: 0:00:18.456931, elapsed time: 0:00:18.456931\n",
      "\n",
      "-------------------------+ NUMBER OF ROWS: 11,135,469 +-------------------------\n",
      "\n",
      "############################### SUBSETTING DATA ################################\n",
      "-----------------------> Step time: 0:00:00.956906, elapsed time: 0:00:19.413837\n",
      "############################### FEATURIZING DATA ###############################\n",
      "-----------------------> Step time: 0:00:08.196619, elapsed time: 0:00:27.610456\n",
      "################################# SORTING DATA #################################\n",
      "-----------------------> Step time: 0:00:04.310154, elapsed time: 0:00:31.920610\n",
      "################################ GROUPING DATA #################################\n",
      "-----------------------> Step time: 0:02:16.333582, elapsed time: 0:02:48.254192\n"
     ]
    }
   ],
   "source": [
    "cpu_results = run_cpu_workflow(data_path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## Total CPU time: 0:02:48.254192 ########################\n",
      "######################## Total GPU time: 0:00:01.827549 ########################\n",
      "########################### Speedup over CPU: 92.065 ###########################\n"
     ]
    }
   ],
   "source": [
    "cpu_runtime = cpu_results[2]\n",
    "gpu_runtime = gpu_results[2]\n",
    "\n",
    "print_message('Total CPU time: {0}'.format(str(cpu_runtime)))\n",
    "print_message('Total GPU time: {0}'.format(str(gpu_runtime)))\n",
    "print_message('Speedup over CPU: {0:.3f}'.format(cpu_runtime / gpu_runtime))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
