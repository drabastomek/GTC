{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA RAPIDS on Azure ML\n",
    "## GTC 2020 DLI WORKSHOP\n",
    "\n",
    "In this notebook we use NYC Taxi dataset to showcase how to scale computations to multiple nodes on Azure ML using Dask. We will also estimate a large XGBoost model.\n",
    "\n",
    "**AUTHORS**\n",
    "* Tom Drabas (Microsoft)\n",
    "* Manuel Reyes Gomez (NVIDIA)\n",
    "\n",
    "**GREATER TEAM**\n",
    "* Joshua Patterson (NVIDIA)\n",
    "* Keith Kraus (NVIDIA)\n",
    "* Brad Rees (NVIDIA)\n",
    "* John Zedlewski (NVIDIA)\n",
    "* Paul Mahler (NVIDIA)\n",
    "* Nick Becker (NVIDIA)\n",
    "* Michael Beaumont (NVIDIA)\n",
    "* Chau Dang (NVIDIA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask_cudf\n",
    "import cudf\n",
    "import cuspatial\n",
    "import os\n",
    "import socket\n",
    "import dask_xgboost as dxgb\n",
    "import distributed\n",
    "import pickle\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "\n",
    "from dask.delayed import delayed\n",
    "\n",
    "from azureml.core import Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Dask cluster and communications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting dask settings...\n",
      "Changes to dask settings\n",
      "-> Setting work-stealing to  False\n",
      "-> Setting scheduler bandwidth to  20\n",
      "Settings updates complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting dask settings...\")\n",
    "dask.config.set({'distributed.scheduler.work-stealing': False})\n",
    "dask.config.set({'distributed.scheduler.bandwidth': 20})\n",
    "print(\"Changes to dask settings\")\n",
    "print(\"-> Setting work-stealing to \", dask.config.get('distributed.scheduler.work-stealing'))\n",
    "print(\"-> Setting scheduler bandwidth to \", dask.config.get('distributed.scheduler.bandwidth'))\n",
    "print(\"Settings updates complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.7.0.5:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.7.0.5:8787/status' target='_blank'>http://10.7.0.5:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.7.0.5:8786' processes=4 threads=4>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = Run.get_context()\n",
    "ip = socket.gethostbyname(socket.gethostname())\n",
    "scheduler = run.get_metrics()[\"scheduler\"]\n",
    "client = distributed.Client(scheduler)\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://10.7.0.5:8786' processes=4 threads=4>\n"
     ]
    }
   ],
   "source": [
    "print(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 14 18:13:58 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00009026:00:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    36W / 250W |    622MiB / 16160MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read, clean and featurize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_OPTIONS = {\n",
    "    'account_name': run.experiment.workspace.datastores['datafiles'].account_name,\n",
    "    'account_key' : run.experiment.workspace.datastores['datafiles'].account_key\n",
    "}\n",
    "\n",
    "protocol  = 'abfs'      # change to 'adl' for gen 1\n",
    "container = 'datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nyctaxi/2015/yellow_tripdata_2015-01.csv',\n",
       " 'nyctaxi/2015/yellow_tripdata_2015-02.csv',\n",
       " 'nyctaxi/2015/yellow_tripdata_2015-03.csv',\n",
       " 'nyctaxi/2015/yellow_tripdata_2015-04.csv',\n",
       " 'nyctaxi/2015/yellow_tripdata_2015-05.csv',\n",
       " 'nyctaxi/2015/yellow_tripdata_2015-06.csv',\n",
       " 'nyctaxi/2015/yellow_tripdata_2015-07.csv',\n",
       " 'nyctaxi/2015/yellow_tripdata_2015-08.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafiles = glob.glob('../../../../../../datafiles/nyctaxi/2015/*')\n",
    "datafiles = ['/'.join(f.split('/')[7:]) for f in datafiles[:8]]\n",
    "datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abfs://datasets/nyctaxi/2015/yellow_tripdata_2015-01.csv',\n",
       " 'abfs://datasets/nyctaxi/2015/yellow_tripdata_2015-02.csv',\n",
       " 'abfs://datasets/nyctaxi/2015/yellow_tripdata_2015-03.csv',\n",
       " 'abfs://datasets/nyctaxi/2015/yellow_tripdata_2015-04.csv',\n",
       " 'abfs://datasets/nyctaxi/2015/yellow_tripdata_2015-05.csv',\n",
       " 'abfs://datasets/nyctaxi/2015/yellow_tripdata_2015-06.csv',\n",
       " 'abfs://datasets/nyctaxi/2015/yellow_tripdata_2015-07.csv',\n",
       " 'abfs://datasets/nyctaxi/2015/yellow_tripdata_2015-08.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [f'{protocol}://{container}/{f}' for f in datafiles]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names that need to be re-mapped\n",
    "remap = {}\n",
    "remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "#create a list of columns & dtypes the df must have\n",
    "must_haves = {\n",
    "    'pickup_datetime': 'datetime64[ms]',\n",
    "    'dropoff_datetime': 'datetime64[ms]',\n",
    "    'passenger_count': 'int32',\n",
    "    'trip_distance': 'float32',\n",
    "    'pickup_longitude': 'float32',\n",
    "    'pickup_latitude': 'float32',\n",
    "    'rate_code': 'int32',\n",
    "    'dropoff_longitude': 'float32',\n",
    "    'dropoff_latitude': 'float32',\n",
    "    'fare_amount': 'float32'\n",
    "}\n",
    "\n",
    "def run_etl(filename, remap, must_haves):\n",
    "    print(os.getcwd())\n",
    "    return cudf.read_csv(filename).map_partitions(clean, remap, must_haves)\n",
    "\n",
    "# helper function which takes a DataFrame partition\n",
    "def clean(df_part, remap, must_haves):    \n",
    "    # some col-names include pre-pended spaces remove & lowercase column names\n",
    "    tmp = {col:col.strip().lower() for col in list(df_part.columns)}\n",
    "    df_part = df_part.rename(tmp)\n",
    "    \n",
    "    # rename using the supplied mapping\n",
    "    df_part = df_part.rename(remap)\n",
    "    \n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col)\n",
    "            continue\n",
    "\n",
    "        if df_part[col].dtype == 'object' and col in ['pickup_datetime', 'dropoff_datetime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "            \n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "    \n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## Number of records: 99,799,189 #########################\n",
      "CPU times: user 1.44 s, sys: 369 ms, total: 1.81 s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "taxi_df = (\n",
    "    dask_cudf\n",
    "    .read_csv(files, storage_options=STORAGE_OPTIONS)\n",
    "    .repartition(npartitions=32)\n",
    "    .map_partitions(clean, remap, must_haves)\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "print(f' Number of records: {len(taxi_df):,} '.center(80, '#'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a list of filter conditions to throw out records with missing or outlier values\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]\n",
    "taxi_df = taxi_df.query(' and '.join(query_frags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add some additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "import numpy as np\n",
    "\n",
    "def haversine_distance_kernel(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, h_distance):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude)):\n",
    "        x_1 = pi / 180 * x_1\n",
    "        y_1 = pi / 180 * y_1\n",
    "        x_2 = pi / 180 * x_2\n",
    "        y_2 = pi / 180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat / 2)**2 + cos(x_1) * cos(x_2) * sin(dlon / 2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "        h_distance[i] = c * r\n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] < 3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m * 2.6) + y + (y // 4) + (c // 4) - 2 * c) % 7\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day'] = df['pickup_datetime'].dt.day\n",
    "    df['diff'] = df['dropoff_datetime'].astype('int32') - df['pickup_datetime'].astype('int32')\n",
    "    \n",
    "    #### BUG -- the below can be done on Titan V and Titan RTX but not P40 (Pascal)\n",
    "#     df['pickup_latitude_r'] = df['pickup_latitude'] // .01 * .01\n",
    "#     df['pickup_longitude_r'] = df['pickup_longitude'] // .01 * .01\n",
    "#     df['dropoff_latitude_r'] = df['dropoff_latitude'] // .01 * .01\n",
    "#     df['dropoff_longitude_r'] = df['dropoff_longitude'] // .01 * .01\n",
    "\n",
    "    #### WORKAROUND\n",
    "    df['pickup_latitude_r']   = (df['pickup_latitude']   / .01).astype('int') / 100.0\n",
    "    df['pickup_longitude_r']  = (df['pickup_longitude']  / .01).astype('int') / 100.0\n",
    "    df['dropoff_latitude_r']  = (df['dropoff_latitude']  / .01).astype('int') / 100.0\n",
    "    df['dropoff_longitude_r'] = (df['dropoff_longitude'] / .01).astype('int') / 100.0\n",
    "\n",
    "    \n",
    "    df = df.drop('pickup_datetime')\n",
    "    df = df.drop('dropoff_datetime')\n",
    "\n",
    "    df = df.apply_rows(haversine_distance_kernel,\n",
    "                       incols=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'],\n",
    "                       outcols=dict(h_distance=np.float32),\n",
    "                       kwargs=dict())\n",
    "\n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                       incols=['day', 'month', 'year'],\n",
    "                       outcols=dict(day_of_week=np.float32),\n",
    "                       kwargs=dict())\n",
    "\n",
    "\n",
    "    df['is_weekend'] = (df['day_of_week']<2).astype(\"int32\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.52 s, sys: 15.5 ms, total: 1.53 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# actually add the features\n",
    "taxi_df = taxi_df.map_partitions(add_features).persist()\n",
    "done = distributed.wait(taxi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered number of rows: 94,375,387\n",
      "CPU times: user 17.3 ms, sys: 3.58 ms, total: 20.8 ms\n",
      "Wall time: 2.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'Filtered number of rows: {len(taxi_df):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some data science!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_partitions(df):\n",
    "    lengths = df.map_partitions(len).compute()\n",
    "    nonempty = [length > 0 for length in lengths]\n",
    "    return df.partitions[nonempty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 561 ms, sys: 7.87 ms, total: 568 ms\n",
      "Wall time: 1.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = taxi_df.query('day < 25').persist()\n",
    "\n",
    "# create a Y_train ddf with just the target variable\n",
    "Y_train = X_train[['fare_amount']].persist()\n",
    "\n",
    "# drop the target variable from the training ddf\n",
    "X_train = X_train[X_train.columns.difference(['fare_amount'])]\n",
    "\n",
    "# this wont return until all data is in GPU memory\n",
    "done = distributed.wait([X_train, Y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## Training examples: 75,030,720 #########################\n",
      "######################### Testing examples: 19,344,667 #########################\n"
     ]
    }
   ],
   "source": [
    "print(f' Training examples: {len(X_train):,} '.center(80, '#'))\n",
    "print(f' Testing examples: {len(X_test):,} '.center(80, '#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 ms, sys: 3.8 ms, total: 30.4 ms\n",
      "Wall time: 5.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "#     'learning_rate'  : 0.3,\n",
    "    'max_depth'      : 3,\n",
    "    'objective'      : 'reg:squarederror',\n",
    "    'subsample'      : 0.5,\n",
    "    'max_leaves'     : 2**4,\n",
    "    'gamma'          : 1,\n",
    "    'n_gpus'         : 1,\n",
    "    'tree_method'    :'gpu_hist'\n",
    "}\n",
    "\n",
    "trained_model = dxgb.train(client, params, X_train, Y_train, num_boost_round=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 698 ms, sys: 5.69 ms, total: 704 ms\n",
      "Wall time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = taxi_df.query('day >= 25').persist()\n",
    "X_test = drop_empty_partitions(X_test)\n",
    "\n",
    "# Create Y_test with just the fare amount\n",
    "Y_test = X_test[['fare_amount']].persist()\n",
    "\n",
    "# Drop the fare amount from X_test\n",
    "X_test = X_test[X_test.columns.difference(['fare_amount'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del taxi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 211 ms, sys: 12.3 ms, total: 223 ms\n",
      "Wall time: 229 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# generate predictions on the test set\n",
    "Y_test['prediction'] = dxgb.predict(client, trained_model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 249 ms, sys: 11 ms, total: 260 ms\n",
      "Wall time: 685 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Y_test['squared_error'] = (Y_test['prediction'] - Y_test['fare_amount'])**2\n",
    "\n",
    "# inspect the results to make sure our calculation looks right\n",
    "Y_test.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.154881782606581\n",
      "CPU times: user 44.7 ms, sys: 0 ns, total: 44.7 ms\n",
      "Wall time: 4.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compute the actual RMSE over the full test set\n",
    "RMSE = Y_test.squared_error.mean().compute()\n",
    "print(math.sqrt(RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(trained_model, open('../saved_models/trained_model_xgboost.mdl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
