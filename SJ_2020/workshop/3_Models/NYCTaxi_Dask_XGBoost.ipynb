{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA RAPIDS on Azure ML\n",
    "## GTC 2020 DLI WORKSHOP\n",
    "\n",
    "In this notebook we use NYC Taxi dataset to showcase how to scale computations to multiple nodes on Azure ML using Dask. We will also estimate a large XGBoost model.\n",
    "\n",
    "**AUTHORS**\n",
    "* Tom Drabas (Microsoft)\n",
    "* Manuel Reyes Gomez (NVIDIA)\n",
    "\n",
    "**GREATER TEAM**\n",
    "* Joshua Patterson (NVIDIA)\n",
    "* Keith Kraus (NVIDIA)\n",
    "* Brad Rees (NVIDIA)\n",
    "* John Zedlewski (NVIDIA)\n",
    "* Paul Mahler (NVIDIA)\n",
    "* Nick Becker (NVIDIA)\n",
    "* Michael Beaumont (NVIDIA)\n",
    "* Chau Dang (NVIDIA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask_cudf\n",
    "import cudf\n",
    "import cuspatial\n",
    "import os\n",
    "import socket\n",
    "import dask_xgboost as dxgb\n",
    "import distributed\n",
    "import pickle\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "\n",
    "from dask.delayed import delayed\n",
    "\n",
    "from azureml.core import Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Dask cluster and communications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting dask settings...\n",
      "Changes to dask settings\n",
      "-> Setting work-stealing to  False\n",
      "-> Setting scheduler bandwidth to  20\n",
      "Settings updates complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting dask settings...\")\n",
    "dask.config.set({'distributed.scheduler.work-stealing': False})\n",
    "dask.config.set({'distributed.scheduler.bandwidth': 20})\n",
    "print(\"Changes to dask settings\")\n",
    "print(\"-> Setting work-stealing to \", dask.config.get('distributed.scheduler.work-stealing'))\n",
    "print(\"-> Setting scheduler bandwidth to \", dask.config.get('distributed.scheduler.bandwidth'))\n",
    "print(\"Settings updates complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.2.0.54:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.2.0.54:8787/status' target='_blank'>http://10.2.0.54:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.2.0.54:8786' processes=1 threads=1>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = Run.get_context()\n",
    "ip = socket.gethostbyname(socket.gethostname())\n",
    "scheduler = run.get_metrics()[\"scheduler\"]\n",
    "client = distributed.Client(scheduler)\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  2 06:02:07 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P40           On   | 000017BD:00:00.0 Off |                    0 |\n",
      "| N/A   20C    P0    50W / 250W |    581MiB / 22919MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read, clean and featurize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_OPTIONS = {\n",
    "    'account_name': run.experiment.workspace.datastores['datafileshare'].account_name,\n",
    "    'account_key' : run.experiment.workspace.datastores['datafileshare'].account_key\n",
    "}\n",
    "\n",
    "protocol  = 'abfs'      # change to 'adl' for gen 1\n",
    "container = 'datafilestore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abfs://datafilestore/data/nyctaxi/2016/yellow_tripdata_2016-01.csv',\n",
       " 'abfs://datafilestore/data/nyctaxi/2016/yellow_tripdata_2016-02.csv',\n",
       " 'abfs://datafilestore/data/nyctaxi/2016/yellow_tripdata_2016-03.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../../../../../../../datafileshare'\n",
    "datafiles = glob.glob(data_path + '/data/nyctaxi/2016/yellow_tripdata_2016-0[1-3].csv')\n",
    "datafiles = ['/'.join(f.split('/')[8:]) for f in datafiles[0:8]]\n",
    "\n",
    "files = [f'{protocol}://{container}/{f}' for f in datafiles]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names that need to be re-mapped\n",
    "remap = {}\n",
    "remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "#create a list of columns & dtypes the df must have\n",
    "must_haves = {\n",
    "    'pickup_datetime': 'datetime64[ms]',\n",
    "    'dropoff_datetime': 'datetime64[ms]',\n",
    "    'passenger_count': 'int32',\n",
    "    'trip_distance': 'float32',\n",
    "    'pickup_longitude': 'float32',\n",
    "    'pickup_latitude': 'float32',\n",
    "    'rate_code': 'int32',\n",
    "    'dropoff_longitude': 'float32',\n",
    "    'dropoff_latitude': 'float32',\n",
    "    'fare_amount': 'float32'\n",
    "}\n",
    "\n",
    "def run_etl(filename, remap, must_haves):\n",
    "    print(os.getcwd())\n",
    "    return cudf.read_csv(filename).map_partitions(clean, remap, must_haves)\n",
    "\n",
    "# helper function which takes a DataFrame partition\n",
    "def clean(df_part, remap, must_haves):    \n",
    "    # some col-names include pre-pended spaces remove & lowercase column names\n",
    "    tmp = {col:col.strip().lower() for col in list(df_part.columns)}\n",
    "    df_part = df_part.rename(tmp)\n",
    "    \n",
    "    # rename using the supplied mapping\n",
    "    df_part = df_part.rename(remap)\n",
    "    \n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col)\n",
    "            continue\n",
    "\n",
    "        if df_part[col].dtype == 'object' and col in ['pickup_datetime', 'dropoff_datetime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "            \n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "    \n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## Number of records: 34,499,859 #########################\n",
      "CPU times: user 688 ms, sys: 50.6 ms, total: 738 ms\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "taxi_df = (\n",
    "    dask_cudf\n",
    "    .read_csv(files, storage_options=STORAGE_OPTIONS)\n",
    "    .repartition(npartitions=32)\n",
    "    .map_partitions(clean, remap, must_haves)\n",
    "    .persist()\n",
    ")\n",
    "\n",
    "print(f' Number of records: {len(taxi_df):,} '.center(80, '#'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove bad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a list of filter conditions to throw out records with missing or outlier values\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]\n",
    "taxi_df = taxi_df.query(' and '.join(query_frags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add some additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "import numpy as np\n",
    "\n",
    "def haversine_distance_kernel(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, h_distance):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude)):\n",
    "        x_1 = pi / 180 * x_1\n",
    "        y_1 = pi / 180 * y_1\n",
    "        x_2 = pi / 180 * x_2\n",
    "        y_2 = pi / 180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat / 2)**2 + cos(x_1) * cos(x_2) * sin(dlon / 2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "        h_distance[i] = c * r\n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] < 3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m * 2.6) + y + (y // 4) + (c // 4) - 2 * c) % 7\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day'] = df['pickup_datetime'].dt.day\n",
    "    df['diff'] = df['dropoff_datetime'].astype('int32') - df['pickup_datetime'].astype('int32')\n",
    "    \n",
    "    #### BUG -- the below can be done on Titan V and Titan RTX but not P40 (Pascal)\n",
    "#     df['pickup_latitude_r'] = df['pickup_latitude'] // .01 * .01\n",
    "#     df['pickup_longitude_r'] = df['pickup_longitude'] // .01 * .01\n",
    "#     df['dropoff_latitude_r'] = df['dropoff_latitude'] // .01 * .01\n",
    "#     df['dropoff_longitude_r'] = df['dropoff_longitude'] // .01 * .01\n",
    "\n",
    "    #### WORKAROUND\n",
    "    df['pickup_latitude_r']   = (df['pickup_latitude']   / .01).astype('int') / 100.0\n",
    "    df['pickup_longitude_r']  = (df['pickup_longitude']  / .01).astype('int') / 100.0\n",
    "    df['dropoff_latitude_r']  = (df['dropoff_latitude']  / .01).astype('int') / 100.0\n",
    "    df['dropoff_longitude_r'] = (df['dropoff_longitude'] / .01).astype('int') / 100.0\n",
    "\n",
    "    \n",
    "    df = df.drop('pickup_datetime')\n",
    "    df = df.drop('dropoff_datetime')\n",
    "\n",
    "    df = df.apply_rows(haversine_distance_kernel,\n",
    "                       incols=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'],\n",
    "                       outcols=dict(h_distance=np.float32),\n",
    "                       kwargs=dict())\n",
    "\n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                       incols=['day', 'month', 'year'],\n",
    "                       outcols=dict(day_of_week=np.float32),\n",
    "                       kwargs=dict())\n",
    "\n",
    "\n",
    "    df['is_weekend'] = (df['day_of_week']<2).astype(\"int32\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.08 s, sys: 31.5 ms, total: 2.11 s\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# actually add the features\n",
    "taxi_df = taxi_df.map_partitions(add_features).persist()\n",
    "done = distributed.wait(taxi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered number of rows: 32,755,964\n",
      "CPU times: user 20.6 ms, sys: 567 µs, total: 21.2 ms\n",
      "Wall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'Filtered number of rows: {len(taxi_df):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do some data science!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_empty_partitions(df):\n",
    "    lengths = df.map_partitions(len).compute()\n",
    "    nonempty = [length > 0 for length in lengths]\n",
    "    return df.partitions[nonempty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 562 ms, sys: 5.24 ms, total: 567 ms\n",
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = taxi_df.query('day < 25').persist()\n",
    "\n",
    "# create a Y_train ddf with just the target variable\n",
    "Y_train = X_train[['fare_amount']].persist()\n",
    "\n",
    "# drop the target variable from the training ddf\n",
    "X_train = X_train[X_train.columns.difference(['fare_amount'])]\n",
    "\n",
    "# this wont return until all data is in GPU memory\n",
    "done = distributed.wait([X_train, Y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## Training examples: 25,895,465 #########################\n"
     ]
    }
   ],
   "source": [
    "print(f' Training examples: {len(X_train):,} '.center(80, '#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.8 ms, sys: 1.14 ms, total: 35 ms\n",
      "Wall time: 9.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "    'max_depth'      : 3,\n",
    "    'objective'      : 'reg:squarederror',\n",
    "    'subsample'      : 0.5,\n",
    "    'max_leaves'     : 2**4,\n",
    "    'gamma'          : 1,\n",
    "    'n_gpus'         : 1,\n",
    "    'tree_method'    :'gpu_hist'\n",
    "}\n",
    "\n",
    "trained_model = dxgb.train(client, params, X_train, Y_train, num_boost_round=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 673 ms, sys: 21.4 ms, total: 694 ms\n",
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = taxi_df.query('day >= 25').persist()\n",
    "X_test = drop_empty_partitions(X_test)\n",
    "\n",
    "# Create Y_test with just the fare amount\n",
    "Y_test = X_test[['fare_amount']].persist()\n",
    "\n",
    "# Drop the fare amount from X_test\n",
    "X_test = X_test[X_test.columns.difference(['fare_amount'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### Testing examples: 6,860,499 ##########################\n"
     ]
    }
   ],
   "source": [
    "del taxi_df\n",
    "print(f' Testing examples: {len(X_test):,} '.center(80, '#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 212 ms, sys: 8.45 ms, total: 221 ms\n",
      "Wall time: 228 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# generate predictions on the test set\n",
    "Y_test['prediction'] = dxgb.predict(client, trained_model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 251 ms, sys: 8.33 ms, total: 259 ms\n",
      "Wall time: 812 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Y_test['squared_error'] = (Y_test['prediction'] - Y_test['fare_amount'])**2\n",
    "\n",
    "# inspect the results to make sure our calculation looks right\n",
    "Y_test.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.288085251263267\n",
      "CPU times: user 62.1 ms, sys: 7.93 ms, total: 70.1 ms\n",
      "Wall time: 5.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# compute the actual RMSE over the full test set\n",
    "RMSE = Y_test.squared_error.mean().compute()\n",
    "print(math.sqrt(RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(trained_model, open('../saved_models/trained_model_xgboost.mdl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
